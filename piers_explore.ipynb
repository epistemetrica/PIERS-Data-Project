{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIERS Data Exploration\n",
    "\n",
    "This notebook addresses missing data and other issues in the PIERS BOL data, and provides basic exploratory analysis of the database. Data is exported from this notebook in form(s) relevant to intended modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #v2.1.3\n",
    "import polars as pl #v0.20.7\n",
    "import plotly_express as px #v0.4.1 \n",
    "import missingno as msno #v0.5.2\n",
    "import datetime as dt\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import time\n",
    "\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()\n",
    "\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience steps:\n",
    "- drop unused columns\n",
    "- create lane_name column\n",
    "- get most commonly used SCAC and name for each carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "imports_path = 'data/raw_parquet/imports/'\n",
    "exports_path = 'data/raw_parquet/exports/'\n",
    "#init lazy dataframes\n",
    "imports_lf = (\n",
    "    pl.scan_parquet(imports_path+'*.parquet', parallel='columns')\n",
    "    #drop unused columns \n",
    "    .select(\n",
    "        'weight',\n",
    "        'weight_unit',\n",
    "        'qty',\n",
    "        'qty_type',\n",
    "        'teus',\n",
    "        #'value_est',\n",
    "        'date',\n",
    "        'container_piece_count',\n",
    "        'commod_short_desc_qty',\n",
    "        'origin_territory',\n",
    "        'origin_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        #'dest_final',\n",
    "        'coast_region',\n",
    "        #'clearing_district',\n",
    "        #'place_receipt',\n",
    "        #'shipper_name',\n",
    "        #'shipper_address',\n",
    "        #'consignee_name',\n",
    "        #'consignee_address',\n",
    "        #'notify_party1_name',\n",
    "        #'notify_party1_address',\n",
    "        #'notify_party2_name',\n",
    "        #'notify_party2_address',\n",
    "        #'commod_desc_raw',\n",
    "        'container_id_marks',\n",
    "        'marks_desc',\n",
    "        'hs_code',\n",
    "        'joc_code',\n",
    "        'commod_short_desc',\n",
    "        'container_ids',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        #'precarrier',\n",
    "        'vessel_id',\n",
    "        #'inbond_code',\n",
    "        #'transport_mode',\n",
    "        'bol_number',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    "    #get most commonly used carrier name for each scac and vise-versa to normalize changes in names and codes \n",
    "    .with_columns(\n",
    "        pl.col('carrier_name').drop_nulls().mode().first().over('carrier_scac')\n",
    "        .alias('unified_carrier_name')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('carrier_scac').drop_nulls().mode().first().over('unified_carrier_name')\n",
    "        .alias('unified_carrier_scac')\n",
    "    )\n",
    ")\n",
    "exports_lf = (\n",
    "    pl.scan_parquet(exports_path+'piers_exports_raw.parquet', parallel='columns') \n",
    "    #drop unused columns\n",
    "    .select(\n",
    "        #'shipper',\n",
    "        #'shipper_address',\n",
    "        'weight',\n",
    "        'weight_unit',\n",
    "        'qty',\n",
    "        'quantity_type',\n",
    "        'teus',\n",
    "        'carrier_name',\n",
    "        'carrier_scac',\n",
    "        'vessel_name',\n",
    "        'voyage_number',\n",
    "        'bol_number',\n",
    "        'vessel_id',\n",
    "        'value_est',\n",
    "        'departure_port_code',\n",
    "        'departure_port_name',\n",
    "        'container_ids',\n",
    "        'container_piece_count',\n",
    "        'coast_region',\n",
    "        #'commod_desc_raw',\n",
    "        'commod_short_desc',\n",
    "        'hs_code',\n",
    "        #'joc_code',\n",
    "        'commod_short_desc_qty',\n",
    "        'date',\n",
    "        #'origin',\n",
    "        'dest_territory',\n",
    "        'dest_region',\n",
    "        'arrival_port_code',\n",
    "        'arrival_port_name',\n",
    "        'direction',\n",
    "        'bol_id',\n",
    "        'year',\n",
    "        'month',\n",
    "        'lane_id'\n",
    "    )\n",
    "    #get lane name \n",
    "    .with_columns(\n",
    "            #find most commonly used departure port name for a given lane_id\n",
    "            pl.col('departure_port_name').drop_nulls().mode().first().over('lane_id').alias('best_departure_port_name'),\n",
    "            #find most commonly used arrival port name for a given lane_id\n",
    "            pl.col('arrival_port_name').drop_nulls().mode().first().over('lane_id').alias('best_arrival_port_name')\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col('best_departure_port_name').cast(pl.Utf8)+' — '+pl.col('best_arrival_port_name').cast(pl.Utf8))\n",
    "            .str.to_titlecase()\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_name')\n",
    "        )\n",
    "        .drop('best_departure_port_name', 'best_arrival_port_name')\n",
    "    #get most commonly used carrier name and scac \n",
    "    .with_columns(\n",
    "        pl.col('carrier_name').drop_nulls().mode().first().over('carrier_scac')\n",
    "        .alias('unified_carrier_name')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('carrier_scac').drop_nulls().mode().first().over('unified_carrier_name')\n",
    "        .alias('unified_carrier_scac')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nulls in volume cols with mean\n",
    "def fill_volume(lf):\n",
    "    '''ad hod function to fill volume columns with their means'''\n",
    "    return (\n",
    "        lf\n",
    "        .with_columns([\n",
    "            pl.col('teus').replace(0,None).fill_null(strategy='mean'),\n",
    "            pl.col('weight').replace(0,None).fill_null(strategy='mean'),\n",
    "            pl.col('qty').replace(0,None).fill_null(strategy='mean')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "#plotly graph inspecting nulls over time by group\n",
    "def nulls_over_time_plotly(data_lf, group_var, time_var, value_var, title=False):\n",
    "    '''\n",
    "    Plots proportion of null values over time by group.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        time_var - str - the name of the time column (e.g., year, month) over which values will be counted\n",
    "        value_var - str - the name of the column containing the variable in question\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        data_lf.select([group_var, time_var, value_var])\n",
    "        #group by, creating null count and non-null count cols\n",
    "        .group_by(group_var, time_var)\n",
    "        .agg([pl.col(value_var).null_count().alias('null_count'),\n",
    "                pl.col(value_var).count().alias('count')])\n",
    "        #compute percent null and fill new column\n",
    "        .with_columns((pl.col('null_count')/(pl.col('count')+pl.col('null_count'))).alias('null_percent'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort(time_var, group_var)\n",
    "    ).collect()\n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df,\n",
    "        x=time_var, y='null_percent',\n",
    "        color=group_var,\n",
    "        title= 'Count of nulls over time by source frame.' if not title else title\n",
    "    )\n",
    "    fig.show()\n",
    "    del df\n",
    "\n",
    "#plotly graph inspecting nulls by group\n",
    "def nulls_by_group_plotly(data_lf, group_var, value_var, title=False):\n",
    "    '''Plots proportion of null values in the given groups'''\n",
    "    df = (\n",
    "        data_lf.select([group_var, value_var])\n",
    "        .group_by(group_var)\n",
    "        .agg([pl.col(value_var).null_count().alias('null_count'),\n",
    "                pl.col(value_var).count().alias('count')])\n",
    "        .with_columns((pl.col('null_count')/(pl.col('count')+pl.col('null_count'))).alias('null_percent'))\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        .sort('null_percent', descending=True)\n",
    "    ).collect()\n",
    "    #plot\n",
    "    fig = px.bar(\n",
    "        data_frame=df,\n",
    "        x=group_var, y='null_percent',\n",
    "        title= 'Null percent by group.' if not title else title\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "#fill nulls over groups given a single unique value per group\n",
    "def fill_nulls_by_group(data_lf, group_vars, val_var):\n",
    "    '''Fills null values by group if and only if the val_var for that group contains exactly one non-null unique value.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_vars - iterable - the names of the columns by which groups will be created\n",
    "        val_var - string - the name of the column in which nulls will be filled\n",
    "    OUTPUT:\n",
    "        filled_lf - the resultant lazyframe \n",
    "    DEPENDS ON:\n",
    "        polars - current version written in polars 0.20.1\n",
    "    '''\n",
    "    filled_lf = (\n",
    "        data_lf.with_columns(\n",
    "            #if the group contains exactly one unique value: \n",
    "            pl.when(pl.col(val_var).drop_nulls().unique(maintain_order=True).len().over(group_vars)==1)\n",
    "            #then fill the group with that value\n",
    "            .then(pl.col(val_var).fill_null(pl.col(val_var).drop_nulls().unique(maintain_order=True).first().over(group_vars)))\n",
    "            #otherwise do nothing\n",
    "            .otherwise(pl.col(val_var))\n",
    "            )\n",
    "        )\n",
    "    return filled_lf\n",
    "\n",
    "def is_one2one(lf, col1, col2):\n",
    "    '''checks if the two pl.LazyFrame columns constitute a 1-1 pairing'''\n",
    "    forward = lf.group_by(col1).agg(pl.col(col2).drop_nulls().n_unique()).select(col2).max().collect().item() == 1\n",
    "    back = lf.group_by(col2).agg(pl.col(col1).drop_nulls().n_unique()).select(col1).max().collect().item() == 1\n",
    "    return (forward and back)\n",
    "\n",
    "def count_unique_by_group(lf, group_vars, val_var):\n",
    "    '''returns a dataframe of unique observations for the value variable over each group'''\n",
    "    df = (\n",
    "        lf.group_by(group_vars)\n",
    "        .agg(\n",
    "            pl.col(val_var).unique().alias('unique_values'),\n",
    "            pl.col(val_var).n_unique().alias('n_unique')\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .sort('n_unique', descending=True)\n",
    "        .collect()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def add_primary_carrier(lf):\n",
    "    '''alternative ad hoc function to find primary carrier for each vessel and indicate cargo sharing'''\n",
    "    lf = (\n",
    "        lf.with_columns(\n",
    "            pl.col('teus').sum()\n",
    "            .over('vessel_id', 'month', 'unified_carrier_scac')\n",
    "            .alias('sum_teus')\n",
    "            )\n",
    "        .with_columns(\n",
    "            pl.col('unified_carrier_scac')\n",
    "            .sort_by('sum_teus', descending=True)\n",
    "            .drop_nulls().first()\n",
    "            .over('vessel_id', 'month')\n",
    "            .alias('vessel_owner')\n",
    "            )\n",
    "        #add bool col if bol is from primary carrier\n",
    "        .with_columns(\n",
    "            (pl.col('unified_carrier_scac')==pl.col('vessel_owner'))\n",
    "            .alias('primary_cargo')\n",
    "            )\n",
    "    )\n",
    "    return lf\n",
    "\n",
    "def sharing_over_time_plotly(data_lf, group_var, include_missing_vessels=True, limit=10, title=False):\n",
    "    '''\n",
    "    Plots proportion of shared cargo over time (months) by group_var.\n",
    "    INPUTS:\n",
    "        data_lf - polars lazyframe containing the relevant data\n",
    "        group_var - str - the name of the column by which to group\n",
    "        include_missing_vessels - bool - default=True, when False, drops missing vessel_ids\n",
    "        title (default=False) - str - the title of the graph\n",
    "    OUTPUT:\n",
    "        a plotly express figure\n",
    "    DEPENDS ON:\n",
    "        polars\n",
    "        plotly express \n",
    "    '''\n",
    "    if not include_missing_vessels:\n",
    "        df = data_lf.drop_nulls('vessel_id')\n",
    "    else:\n",
    "        df = data_lf\n",
    "    \n",
    "    df = (\n",
    "        #select relevant columns\n",
    "        df.select([group_var, 'month', 'primary_cargo', 'teus'])\n",
    "        #sum teus over each group-month-shared \n",
    "        .group_by(group_var, 'month')\n",
    "        .agg(\n",
    "            (pl.col('teus')*pl.col('primary_cargo')).sum().alias('total_primary'),\n",
    "            pl.col('teus').sum().alias('total_teus')\n",
    "        )\n",
    "        #create proportion shared\n",
    "        .with_columns((1-(pl.col('total_primary')/pl.col('total_teus'))).alias('prop_shared'))\n",
    "        #cast group col to string to allow sensible ordering of legend\n",
    "        .cast({group_var:pl.Utf8})\n",
    "        #sort by date (to allow proper visualization of lines) and group (for legend ordering) \n",
    "        .sort('month')\n",
    "    ).collect()\n",
    "\n",
    "    #limit categories\n",
    "    top_groups = (\n",
    "        data_lf.group_by(group_var)\n",
    "        .agg(pl.col('teus').sum())\n",
    "        .sort('teus', descending=True)\n",
    "        .select(group_var)\n",
    "        .limit(limit)\n",
    "        .collect()\n",
    "        .to_series()\n",
    "        .cast(pl.Utf8)\n",
    "    )\n",
    "    \n",
    "    #plot\n",
    "    fig = px.line(\n",
    "        data_frame=df.filter(pl.col(group_var).is_in(top_groups)).with_columns(pl.col('month').str.to_datetime('%Y%m')),\n",
    "        x='month', y='prop_shared',\n",
    "        color=group_var,\n",
    "        title= 'Proportion of shared cargo over time.' if not title else title,\n",
    "        labels={\n",
    "            'prop_shared':'Proportion of cargo from non-primary carrier',\n",
    "            'month':'Month'\n",
    "        }\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Major areas of missing data include:\n",
    "- TEU and other volume data, especially prior to 2015\n",
    "- vessel_id (IMO codes), especially in certain date windows\n",
    "- Carriers are sometimes codes as \"BULK\" instead of the SCAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "#MSNO provides a matrix visualization fo missing data; to deploy, comment out the %%script line. \n",
    "print('Missing Values Matrix for Exports. \\nThe vertical axis corresponds to date, with earliest at the top.  \\nNote the daily aggregation counts the row as non-null if any value that day is non-null.')\n",
    "msno.matrix(\n",
    "    exports_lf\n",
    "    #convert all columns except date to boolean False=null\n",
    "    .with_columns(pl.all().exclude('date').is_not_null())\n",
    "    #aggregate by day to stay within visualization limitations \n",
    "    .group_by('date').sum()\n",
    "    .sort('date')\n",
    "    #convert False values back to null \n",
    "    .with_columns(pl.all().replace(0,None))\n",
    "    .collect()\n",
    "    .to_pandas()\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "print('Missing Values Matrix for Imports. \\nThe vertical axis corresponds to date, with earliest at the top. \\nNote the daily aggregation counts the row as non-null if any value that day is non-null.')\n",
    "msno.matrix(\n",
    "    imports_lf\n",
    "    #convert all columns except date to boolean False=null\n",
    "    .with_columns(pl.all().exclude('date').is_not_null())\n",
    "    #aggregate by day to stay within visualization limitations \n",
    "    .group_by('date').sum()\n",
    "    .sort('date')\n",
    "    #convert False values back to null \n",
    "    .with_columns(pl.all().replace(0,None))\n",
    "    .collect()\n",
    "    .to_pandas()\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero values in TEU and other volume columns\n",
    "\n",
    "For the time being, we fill missing volume data with the mean from non-zero observations. This step is expected to change as beter metadata is gathered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_lf = fill_volume(imports_lf)\n",
    "exports_lf = fill_volume(exports_lf)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Vessel and Voyage IDs\n",
    "\n",
    "Identifying unique vessels and voyages is critical to studying how long each vessel remains in port, how often carriers utilize alliances (e.g., what proportion of a vessel's cargo is from another carrier), and other quality metrics. The issue is especially concerning given the high proportion (~30% in some time periods) of missing data, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.concat(\n",
    "    [imports_lf.select('direction', 'month', 'vessel_id'),\n",
    "    exports_lf.select('direction', 'month', 'vessel_id')]\n",
    ")\n",
    "\n",
    "nulls_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    group_var='direction',\n",
    "    time_var='month',\n",
    "    value_var='vessel_id',\n",
    "    title='Proportion of Missing Vessel IDs over time (original data).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.concat(\n",
    "    [imports_lf.select('direction', 'month', 'vessel_name'),\n",
    "    exports_lf.select('direction', 'month', 'vessel_name')]\n",
    ")\n",
    "\n",
    "nulls_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    group_var='direction',\n",
    "    time_var='month',\n",
    "    value_var='vessel_name',\n",
    "    title='Proportion of Missing Vessel Names over time (original data).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Carrier\n",
    "\n",
    "For any given voyage, we assign primary carrier based on the proportion of TEUs from each carrier imported/exported on that voyage (max proportion is primary carrier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add primary carrier and shared cargo columns to lfs\n",
    "imports_lf = add_primary_carrier(imports_lf)\n",
    "exports_lf = add_primary_carrier(exports_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.concat([imports_lf,exports_lf], how='diagonal')\n",
    "\n",
    "sharing_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    include_missing_vessels=False,\n",
    "    group_var='direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=exports_lf,\n",
    "    group_var='departure_port_name',\n",
    "    title='Proportion of shared cargo over time (Top 10 export ports by total TEUs).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=exports_lf,\n",
    "    group_var='departure_port_name',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time (Top 10 export ports by total TEUs).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=exports_lf,\n",
    "    group_var='lane_name',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time (Top 10 export lanes by total TEUs).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=imports_lf,\n",
    "    group_var='lane_name',\n",
    "    title='Proportion of shared cargo over time (Top 10 import lanes by total TEUs).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=imports_lf,\n",
    "    group_var='vessel_owner',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time (Top 10 Vessel Owners by total TEUs imported).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_over_time_plotly(\n",
    "    data_lf=exports_lf,\n",
    "    group_var='vessel_owner',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time (Top 10 Vessel Owners by total TEUs exported).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by MSC and Maersk controlled voyages\n",
    "lf = (\n",
    "    imports_lf.filter(\n",
    "        pl.col('vessel_owner').is_in(['MEDU','MAEU']),\n",
    "        pl.col('unified_carrier_scac').is_in(['MEDU','MAEU'])\n",
    "    )\n",
    ")\n",
    "sharing_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    group_var='lane_name',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time for MSC and Maersk vessels (Top 10 import lanes by total TEUs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by MSC and Maersk controlled voyages\n",
    "lf = (\n",
    "    exports_lf.filter(\n",
    "        pl.col('vessel_owner').is_in(['MSCU','MLSL']),\n",
    "        pl.col('unified_carrier_scac').is_in(['MSCU','MLSL'])\n",
    "    )\n",
    ")\n",
    "sharing_over_time_plotly(\n",
    "    data_lf=lf,\n",
    "    group_var='lane_name',\n",
    "    include_missing_vessels=False,\n",
    "    title='Proportion of shared cargo over time for MSC and Maersk vessels (Top 10 export lanes by total TEUs).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'BULK' Carrier SCAC\n",
    "\n",
    "Many bols show \"BULK\" in place of carrier code, and at least some bols also show legitimate carrier names. We'll need to clean this up. Perhaps assign carrier code based on carrier name where possible and null values for the reminder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying port visits \n",
    "\n",
    "Date data for each BOL is spread over multiple dates for many vessel-port pairs, possibly indicating that 'date' refers to the day on which the bol was processed through customs rather than the actual arrival or departure date of the ship carrying that bol. Whatever the cause, our date data is noisy.\n",
    "\n",
    "To correct for this, we apply a Heirachical Density Based Clustering Algorithm with Noise (HDBSCAN) to identify clusters of cargo volumes imported(exported) on similar dates, making the assumption that a dense cluster of TEUs processed on or near a given date indictates the date of arrival(departure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_dates(lf, direction, samples=None):\n",
    "    '''\n",
    "    Finds arrival/departure date using the following algorithm:\n",
    "        1. Create 1-D dataframe of dates for each vessel-port pair, \n",
    "            with one date occurance per TEU processed on that date\n",
    "        2. Find clusers of dates using SciKitLearn's HDBSCAN\n",
    "        3. Assign mode date of each cluster as the arrival/departure date\n",
    "        4. Assign any bols with dates occuring between the modes as arriving/departing\n",
    "            on the date of the preceeding mode.\n",
    "        5. Join imputed arrival/departure dates into main lazyframe. \n",
    "    INPUTS\n",
    "        lf - a polars LazyFrame containing the relevant data\n",
    "        direction - 'imports' or 'exports' - indicating the source data\n",
    "        samples - int - number of random samples \n",
    "    OUTPUTS\n",
    "        lf - the original lazyframe with imputed dates \n",
    "    '''\n",
    "    #create vessel_port_pair columns in main lf\n",
    "    lf = (\n",
    "        lf.with_columns(\n",
    "            (pl.col('vessel_id').cast(pl.Utf8)+'_'+pl.col('arrival_port_code').cast(pl.Utf8))\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('vessel_port_pair')\n",
    "        )\n",
    "    )\n",
    "    #collect relevant columns from lf\n",
    "    begin_collect = time.time()\n",
    "    df = (\n",
    "        lf.group_by('date', 'vessel_port_pair')\n",
    "        #get sum of TEUs on each date \n",
    "        .agg(pl.col('teus').sum().alias('sum_teus'))\n",
    "        #drop missing vessel-port pairs\n",
    "        .drop_nulls(subset='vessel_port_pair')\n",
    "        #sort by date\n",
    "        .sort('date')\n",
    "        .collect()\n",
    "    )\n",
    "    print('clustering data collected; time = {:.2f} minutes'.format((time.time() - begin_collect)/60))\n",
    "    #initialize variables\n",
    "    samples=samples \n",
    "    if samples:\n",
    "        pairs = df.select('vessel_port_pair').sample(samples).to_series()\n",
    "    else:\n",
    "        pairs = df.select('vessel_port_pair').to_series()\n",
    "    pairs_df = pl.DataFrame()\n",
    "    #loop through vessel-port pairs\n",
    "    print('Looping through vessel-port pairs')\n",
    "    for i in range(len(pairs)):\n",
    "        if i%1000 == 0:\n",
    "            begin_block = time.time()\n",
    "        pair = pairs[i]\n",
    "        #make single-column dataframe of dates where each date corresponds to a single TEU that arrived on that day \n",
    "        pair_1d = (\n",
    "            df.filter(pl.col('vessel_port_pair')==pair)\n",
    "            .select('date', pl.col('sum_teus').round())\n",
    "            #explode dates by each teu \n",
    "            .select(pl.exclude('sum_teus').repeat_by('sum_teus').explode())\n",
    "        )\n",
    "        #find minimum number of occurances of a single date (needed for HDBSCAN param)\n",
    "        min_sample = pair_1d.group_by('date').agg(pl.col('date').count().alias('count')).min().row(0)[1]\n",
    "        #skip empty pairs\n",
    "        if min_sample == 0:\n",
    "            continue\n",
    "        #skip vessel_port pairs with less than 2 dates\n",
    "        if len(pair_1d) < 2:\n",
    "            continue\n",
    "        #instantiate clusterer\n",
    "        clusterer = HDBSCAN(min_cluster_size=20, min_samples=min_sample) #we need to find a dynamic way of seleting these parameters\n",
    "        #get clusters\n",
    "        clusterer.fit(pair_1d)\n",
    "        #add back to pair_1d\n",
    "        pair_df = (\n",
    "            pair_1d\n",
    "            #add cluster column\n",
    "            .with_columns(\n",
    "                pl.Series(name='cluster', values=clusterer.labels_)\n",
    "            )\n",
    "            #add date_arrival column\n",
    "            .with_columns(\n",
    "                    #when date matches the mode of each cluster\n",
    "                    pl.when(pl.col('date') == pl.col('date').mode().first().over('cluster'))\n",
    "                    #fill with that date, otherwise fill with null\n",
    "                    .then(pl.col('date'))\n",
    "                    .otherwise(pl.lit(None))\n",
    "                    #forward fill the arrival date to the mode of next cluster\n",
    "                    .forward_fill()\n",
    "                    #backward fill the first part of first cluster\n",
    "                    .backward_fill()\n",
    "                    #name column\n",
    "                    .alias('date_imputed')\n",
    "                )\n",
    "            #groupby date to simplify\n",
    "            .group_by('date')\n",
    "            .agg(pl.col('date_imputed').first())\n",
    "            #add pair label\n",
    "            .with_columns(pl.lit(pair).alias('vessel_port_pair').cast(pl.Categorical))\n",
    "        )\n",
    "        #init or concat pairs_df\n",
    "        if i == 0:\n",
    "            pairs_df = pair_df   \n",
    "        else:\n",
    "            pairs_df = pl.concat([pairs_df,pair_df], how='vertical')\n",
    "        #print status update\n",
    "        if (i != 0) and ((i+1)%1000 == 0):\n",
    "            print('{} pairs clustered. The previous 100 pairs took {:.2f} hours.'.format(i+1, (time.time()-begin_block)/3600))\n",
    "    #rename imputed dates based on direction\n",
    "    if direction=='import':\n",
    "        pairs_df = pairs_df.rename({'date_imputed': 'date_arrival'})\n",
    "    elif direction=='export':\n",
    "        pairs_df = pairs_df.rename({'date_imputed': 'date_departure'})\n",
    "    else:\n",
    "        raise Exception('direction must equal \"import\" or \"export\"')\n",
    "    #join imputed dates to main lf\n",
    "    pairs_lf = pairs_df.lazy()\n",
    "    lf = (\n",
    "        lf.join(pairs_lf, on=['date', 'vessel_port_pair'], how='left')\n",
    "    )\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping \n",
    "start = time.time()\n",
    "#apply clustering function\n",
    "imports_lf = cluster_dates(imports_lf, direction='import', samples=None)\n",
    "exports_lf = cluster_dates(exports_lf, direction='export', samples=None)\n",
    "\n",
    "runtime = time.time() - start\n",
    "print('Total time to cluster dates: {:.2f} hours'.format(runtime/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking Lists (commodity codes, etc)\n",
    "\n",
    "List-like columns include:\n",
    "- container_id_marks\n",
    "- marks_desc\n",
    "- hs_code\n",
    "- job_code\n",
    "- commod_desc_raw\n",
    "- container_ids\n",
    "- commod_short_desc_qty\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_hscodes(lf):\n",
    "    '''\n",
    "    Expands the dataset into long form where each row corresponds to only a single commodity (HS Code) within each BOL. \n",
    "        Assumes that the raw data are effectively grouped by BOL, resulting in hs_code column sometimes containing more than one code;\n",
    "        this function undoes that group aggregation. \n",
    "    INPUTS\n",
    "        lf - a polars lazyframe containing the relevant data\n",
    "    OUTPUTS\n",
    "        lf - a polars lazyframe containing the long form data\n",
    "    '''\n",
    "    #instantiate index\n",
    "    index = pl.arange(0, lf.select(pl.len()).collect().item(), eager=True)\n",
    "    #explode hs_codes \n",
    "    lf = (\n",
    "        lf\n",
    "        #set nulls to 0 to prevent data loss in later steps\n",
    "        .with_columns(pl.col('hs_code').replace(None, 'missing'))\n",
    "        .with_columns(\n",
    "            #split hs codes into lists\n",
    "            pl.col('hs_code').str.split(by=' '),\n",
    "            #create pseudo-index col\n",
    "            pl.Series(name='index', values=index)\n",
    "            )\n",
    "        #explode hs_codes\n",
    "        .explode('hs_code')\n",
    "        #drop rows with empty strings (these result from more than one space between hs_codes in the raw data)\n",
    "        .with_columns(pl.col('hs_code').replace('', None))\n",
    "        .drop_nulls('hs_code')\n",
    "        #distribute total volumes evenly across HS Codes -- NOTE this step may change as better metadata is collected\n",
    "        .with_columns(\n",
    "            (pl.col('teus')/(pl.col('index').count().over('index'))),\n",
    "            (pl.col('qty')/(pl.col('index').count().over('index'))),\n",
    "            (pl.col('weight')/(pl.col('index').count().over('index'))),\n",
    "            #replace originally-null values\n",
    "            pl.col('hs_code').replace('missing', None)\n",
    "        )\n",
    "        #drop index\n",
    "        .drop('index')\n",
    "    )\n",
    "    return lf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode hs codes\n",
    "imports_lf = explode_hscodes(imports_lf)\n",
    "exports_lf = explode_hscodes(exports_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Clean data to parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get years\n",
    "years = pl.arange(2005,2024, eager=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for year in years:\n",
    "    print('Collecting {} dataframe...'.format(year))\n",
    "    df = (\n",
    "        imports_lf\n",
    "        .filter(pl.col('year')==year)\n",
    "        .collect()\n",
    "    )\n",
    "    print('Writing {} data to parquet...'.format(year))\n",
    "    df.write_parquet(file='data/clean_parquet/imports/piers_imports_'+str(year)+'.parquet')\n",
    "print('Imports data written to parquet')\n",
    "runtime = time.time() - start\n",
    "print('Total time to write imports: {:.2f} hours'.format(runtime/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('Collecting exports data...')\n",
    "df = exports_lf.collect()\n",
    "print('Writing exports data to parquet...')\n",
    "df.write_parquet('data/clean_parquet/exports/piers_exports_clean.parquet')\n",
    "del df\n",
    "print('Exports data written to parquet.')\n",
    "runtime = time.time() - start\n",
    "print('Total time to write exports: {:.2f} hours'.format(runtime/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - begin\n",
    "print('Total runtime: {:.2f} hours'.format(end/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
