{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis of Import Data\n",
    "\n",
    "This notebook explores the PIERS Bill of Lading data, obtained from S&P's Global Trade Analytics Suite. See the README.md file for more info on the overall project, data pre-processing, and column definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "imports_path = 'data/clean/imports/'\n",
    "exports_path = 'data/clean/exports/'\n",
    "\n",
    "#get schema and col names\n",
    "imports_schema = pl.read_parquet_schema(source= imports_path+'piers_imports_2005.parquet')\n",
    "imports_colnames = imports_schema.keys()\n",
    "exports_schema = pl.read_parquet_schema(source=exports_path+'piers_exports_complete.parquet')\n",
    "exports_colnames = exports_schema.keys()\n",
    "\n",
    "#init lazy dataframes\n",
    "imports_lzdf = pl.scan_parquet(imports_path+'*.parquet', parallel='columns')\n",
    "exports_lzdf = pl.scan_parquet(exports_path+'piers_exports_complete.parquet', parallel='columns')\n",
    "\n",
    "#get number of observations\n",
    "imports_n = imports_lzdf.select(pl.count()).collect().item()\n",
    "exports_n = exports_lzdf.select(pl.count()).collect().item()\n",
    "print('The imports dataset has {:,} unique observations and {} columns.'.format(imports_n, len(imports_schema)))\n",
    "print('The exports dataset has {:,} unique observations and {} columns.'.format(exports_n, len(exports_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view head of imports dataframe \n",
    "imports_lzdf.limit(n=3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view head of exports dataframe \n",
    "exports_lzdf.limit(n=3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init df and get stats labels column\n",
    "imports_summarystats_df = imports_lzdf.select(pl.first()).collect().describe().select(pl.first()).to_pandas()\n",
    "#loop through columns and get descriptive stats\n",
    "for column in imports_colnames:\n",
    "    imports_summarystats_df[column] = imports_lzdf.select(pl.col(column)).collect().describe().select(column).to_pandas()\n",
    "#display\n",
    "imports_summarystats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE at the moment the exports dataset may fit in memory, in which case the below code could be accomplished more efficiently\n",
    "#by executing the following line; however, the main code below should run even when the dataset does not fit in memory. \n",
    "#exports_lzdf.collect().describe()\n",
    "\n",
    "#init df and get stats labels column\n",
    "exports_summarystats_df = exports_lzdf.select(pl.first()).collect().describe().select(pl.first()).to_pandas()\n",
    "#loop through columns and get descriptive stats\n",
    "for column in exports_colnames:\n",
    "    exports_summarystats_df[column] = exports_lzdf.select(pl.col(column)).collect().describe().select(column).to_pandas()\n",
    "#display\n",
    "exports_summarystats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate BOLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count unique bol_scac IDs\n",
    "import_bols_unique_n = imports_lzdf.select(pl.col('bol_id')).unique().select(pl.count()).collect().item()\n",
    "export_bols_unique_n = exports_lzdf.select(pl.col('bol_id')).unique().select(pl.count()).collect().item()\n",
    "\n",
    "print('{:,} out of {:,} rows in the imports dataset contain duplicated BoLs.'.format(imports_n-import_bols_unique_n, imports_n))\n",
    "print('{:,} out of {:,} rows in the imports dataset contain duplicated BoLs.'.format(exports_n-export_bols_unique_n, exports_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible reasons:\n",
    "- bol includes items on more than one ship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "194.8k import bols share a bol-scac ID but have different vessel IDs. \n",
    "664.1k export bols share a bol-scac ID but have different vessel IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entries with zero weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf[pldf.weight == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value and Volumes by Year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get year col\n",
    "pldf['year'] = pldf.date_arrival.dt.to_period('Y')\n",
    "#group value and volume by year\n",
    "activityacrosstime_df = pldf[['year', 'teus', 'value_est']].groupby('year').sum()\n",
    "#plot\n",
    "sns.barplot(data=activityacrosstime_df, x='year', y='value_est');\n",
    "plt.title('Total Value of Imports Over Time')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "sns.barplot(data=activityacrosstime_df, x='year', y='teus');\n",
    "plt.title('Total Volume (TEUs) of Imports Over Time')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess value and volume records weren't kept before ~2015 ?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group value and volume by year\n",
    "until2012_df = pldf[pldf['year']< pd.Period(2013)]\n",
    "until2012_df = until2012_df[['year', 'teus', 'value_est']].groupby('year').sum()\n",
    "#plot\n",
    "sns.barplot(data=until2012_df, x='year', y='teus');\n",
    "plt.title('Total Volume (TEUs) of Imports Over Time')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must be missing data here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get year col\n",
    "pldf['year'] = pldf.date_arrival.dt.to_period('Y')\n",
    "#group value and volume by year\n",
    "activityacrosstime_df = pldf[['year', 'container_piece_count']].groupby('year').sum()\n",
    "#plot\n",
    "sns.barplot(data=activityacrosstime_df, x='year', y='container_piece_count');\n",
    "plt.title('Total Volume (Container Piece Count) of Imports Over Time')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carriers Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriersovertime_df = pldf[['year', 'carrier_scac']].groupby('year').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=carriersovertime_df, x='year', y='carrier_scac');\n",
    "plt.title('Number of Unique Carriers Over Time');\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there really a spike in carriers in 2010 and 2012? Or does this indicate changes in the way SCAC codes are assigned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market share of the 50 largest carriers by estimated value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get largest carriers\n",
    "carriers_df = pldf[pldf.year > pd.Period(2014)]\n",
    "carriers_df = carriers_df[['year', 'carrier_scac', 'value_est']].groupby(['year', 'carrier_scac']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.columns = ['value_usd']\n",
    "carriers_df.sort_values('value_usd', ascending=False, inplace=True)\n",
    "carriers_df.sort_values('year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.reset_index(inplace=True)\n",
    "carriers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
