{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIERS Container BOL Data ETL \n",
    "\n",
    "This notebook builds an ETL pipeline for S&P Global's PIERS data. Data is extracted from CSV files downloaded from the Global Trade Analytics Suite, assigned appropriate datatypes, concatendated into a single dataframe, and loaded to an Apache Parquet file for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform\n",
    "\n",
    "Read from csv into a pandas dataframe with appropriate dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for future optimization: build a dictionary of column dtypes and assign within read_csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path\n",
    "path = 'data/raw/'\n",
    "#get list of data files, ignoring any hidden files in directory \n",
    "datafiles = [file for file in os.listdir(path) if not file.startswith('.')]\n",
    "#init filenumber\n",
    "filenumber = 1\n",
    "#define new col names\n",
    "import_colnames_dict = {'Weight': 'weight',\n",
    "                        'Weight Unit': 'weight_unit',\n",
    "                        'Quantity': 'qty',\n",
    "                        'Quantity Type': 'qty_type',\n",
    "                        'TEUs': 'teus',\n",
    "                        'Estimated Value': 'value_est',\n",
    "                        'Arrival Date': 'date_arrival',\n",
    "                        'Container Piece Count': 'container_piece_count',\n",
    "                        'Quantity of Commodity Short Description': 'commod_short_desc_qty',\n",
    "                        'Territory of Origin': 'origin_territory',\n",
    "                        'Region of Origin': 'origin_region',\n",
    "                        'Port of Arrival Code': 'arrival_port_code',\n",
    "                        'Port of Arrival': 'arrival_port_name',\n",
    "                        'Port of Departure Code': 'departure_port_code',\n",
    "                        'Port of Departure': 'departure_port_name',\n",
    "                        'Final Destination': 'dest_final',\n",
    "                        'Coastal Region': 'coast_region',\n",
    "                        'Clearing District': 'clearing_district',\n",
    "                        'Place of Receipt': 'place_receipt',\n",
    "                        'Shipper': 'shipper_name',\n",
    "                        'Shipper Address': 'shipper_address',\n",
    "                        'Consignee': 'consignee_name',\n",
    "                        'Consignee Address': 'consignee_address',\n",
    "                        'Notify Party': 'notify_party1_name',\n",
    "                        'Notify Party Address': 'notify_party1_address',\n",
    "                        'Also Notify Party': 'notify_party2_name',\n",
    "                        'Also Notify Party Address': 'notify_party2_address',\n",
    "                        'Raw Commodity Description': 'commod_desc_raw',\n",
    "                        'Marks Container Number': 'container_id_marks',\n",
    "                        'Marks Description': 'marks_desc',\n",
    "                        'HS Code': 'hs_code',\n",
    "                        'JOC Code': 'joc_code',\n",
    "                        'Commodity Short Description': 'commod_short_desc',\n",
    "                        'Container Number': 'container_ids',\n",
    "                        'Carrier': 'carrier_name',\n",
    "                        'SCAC': 'carrier_scac',\n",
    "                        'Vessel Name': 'vessel_name',\n",
    "                        'Voyage Number': 'vessel_id',\n",
    "                        'Pre Carrier': 'precarrier',\n",
    "                        'IMO Number': 'imo_num',\n",
    "                        'Inbond Code': 'inbond_code',\n",
    "                        'Mode of Transport': 'transport_mode',\n",
    "                        'Bill of Lading Number': 'bol_id'}\n",
    "#define dtypes\n",
    "import_dtype_dict = {'Weight': 'float64',\n",
    "            'Weight Unit': 'category',\n",
    "            'Quantity': 'float64',\n",
    "            'Quantity Type': 'category',\n",
    "            'TEUs': 'float64',\n",
    "            'Estimated Value': 'float64',\n",
    "            'Arrival Date': 'int64',\n",
    "            'Container Piece Count': 'int64',\n",
    "            'Quantity of Commodity Short Description': 'object',\n",
    "            'Territory of Origin': 'category',\n",
    "            'Region of Origin': 'category',\n",
    "            'Port of Arrival Code': 'category',\n",
    "            'Port of Arrival': 'category',\n",
    "            'Port of Departure Code': 'category',\n",
    "            'Port of Departure': 'category',\n",
    "            'Final Destination': 'category',\n",
    "            'Coastal Region': 'category',\n",
    "            'Clearing District': 'category',\n",
    "            'Place of Receipt': 'category',\n",
    "            'Shipper': 'object',\n",
    "            'Shipper Address': 'object',\n",
    "            'Consignee': 'object',\n",
    "            'Consignee Address': 'object',\n",
    "            'Notify Party': 'object',\n",
    "            'Notify Party Address': 'object',\n",
    "            'Also Notify Party': 'object',\n",
    "            'Also Notify Party Address': 'object',\n",
    "            'Raw Commodity Description': 'object',\n",
    "            'Marks Container Number': 'object',\n",
    "            'Marks Description': 'object',\n",
    "            'HS Code': 'category',\n",
    "            'JOC Code': 'category',\n",
    "            'Commodity Short Description': 'object',\n",
    "            'Container Number': 'object',\n",
    "            'Carrier': 'category',\n",
    "            'SCAC': 'category',\n",
    "            'Vessel Name': 'object',\n",
    "            'Voyage Number': 'object',\n",
    "            'Pre Carrier': 'float64',\n",
    "            'IMO Number': 'float64',\n",
    "            'Inbond Code': 'float64',\n",
    "            'Mode of Transport': 'category',\n",
    "            'Bill of Lading Number': 'object'}\n",
    "#define category variable cols\n",
    "catcols = ['weight_unit', 'qty_type', 'origin_territory', 'origin_region', 'arrival_port_code', \n",
    "           'arrival_port_name', 'departure_port_code', 'departure_port_name', 'dest_final', 'coast_region', \n",
    "           'clearing_district', 'place_receipt', 'hs_code', 'joc_code', 'carrier_name', 'carrier_scac', \n",
    "           'transport_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSV files...\n",
      " Files to process:  39 \n",
      "\n",
      "Extracting file  1 ...\n",
      "Extraction complete.\n",
      " Time: 29.970886945724487 sec \n",
      "\n",
      "Extracting file  2 ...\n",
      "Extraction complete.\n",
      " Time: 37.02122783660889 sec \n",
      "\n",
      "Extracting file  3 ...\n",
      "Extraction complete.\n",
      " Time: 19.829867124557495 sec \n",
      "\n",
      "Extracting file  4 ...\n",
      "Extraction complete.\n",
      " Time: 33.27042508125305 sec \n",
      "\n",
      "Extracting file  5 ...\n",
      "Extraction complete.\n",
      " Time: 34.62395882606506 sec \n",
      "\n",
      "Extracting file  6 ...\n",
      "Extraction complete.\n",
      " Time: 33.91834902763367 sec \n",
      "\n",
      "Extracting file  7 ...\n",
      "Extraction complete.\n",
      " Time: 21.45353078842163 sec \n",
      "\n",
      "Extracting file  8 ...\n",
      "Extraction complete.\n",
      " Time: 41.110548973083496 sec \n",
      "\n",
      "Extracting file  9 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting file \u001b[39m\u001b[38;5;124m'\u001b[39m, filenumber, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#read csv with appropriate dtypes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m file_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_dtype_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#rename columns\u001b[39;00m\n\u001b[1;32m     10\u001b[0m file_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mimport_colnames_dict, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1065\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1104\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1189\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py:509\u001b[0m, in \u001b[0;36mCategoricalDtype.construct_array_type\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    506\u001b[0m     combined_hashed \u001b[38;5;241m=\u001b[39m combine_hash_arrays(\u001b[38;5;28miter\u001b[39m(cat_array), num_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(cat_array))\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbitwise_xor\u001b[38;5;241m.\u001b[39mreduce(combined_hashed)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_array_type\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m type_t[Categorical]:\n\u001b[1;32m    511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    Return the array type associated with this dtype.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    type\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Extracting CSV files...\\n', 'Files to process: ', len(datafiles), '\\n')\n",
    "\n",
    "#extract from csv to clean dataframes and concat\n",
    "for filename in datafiles:\n",
    "    start = time.time()\n",
    "    print('Extracting file ', filenumber, '...')\n",
    "    #read csv with appropriate dtypes\n",
    "    file_df = pd.read_csv(path+filename, dtype=import_dtype_dict)\n",
    "    #rename columns\n",
    "    file_df.rename(columns=import_colnames_dict, inplace=True)\n",
    "    #unpack strings to list objects\n",
    "    file_df.container_ids = file_df.container_ids.str.split()\n",
    "    file_df.commod_short_desc_qty = file_df.commod_short_desc_qty.str.split(pat=';')\n",
    "    file_df.commod_short_desc = file_df.commod_short_desc.str.split(pat=',')\n",
    "    #recast dates to datetime \n",
    "    file_df.date_arrival = pd.to_datetime(file_df.date_arrival.astype(str), format='%Y%m%d') \n",
    "    #concat to or create main imports df\n",
    "    if 'imports_df' in locals():\n",
    "        #create category unions and assign union to each df col\n",
    "        for col in catcols:\n",
    "            catunion = pd.api.types.union_categoricals([imports_df[col], file_df[col]])\n",
    "            imports_df[col] = pd.Categorical(imports_df[col], categories=catunion.categories)\n",
    "            file_df[col] = pd.Categorical(file_df[col], categories=catunion.categories)\n",
    "        #concat to main df\n",
    "        imports_df = pd.concat([imports_df, file_df])\n",
    "    else:\n",
    "        imports_df = file_df \n",
    "    del file_df\n",
    "    end = time.time()\n",
    "    print('Extraction complete.\\n', 'Time: {} sec \\n'.format(end-start))\n",
    "    filenumber += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect output \n",
    "display(imports_df.head())\n",
    "imports_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet file\n",
    "imports_df.to_parquet('data/piers_imports.parquet', index=False, engine='fastparquet') #requires fastparquet dependency  \n",
    "\n",
    "#delete imports df\n",
    "del imports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_parquet('data/piers_imports.parquet', engine='fastparquet')\n",
    "imports_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
