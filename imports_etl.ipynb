{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "#ini Dask client\n",
    "cluster = LocalCluster(n_workers=10)\n",
    "client = Client(cluster)\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- read csvs\n",
    "    - set datatypes and column names during read\n",
    "- \n",
    "- concat dataframes\n",
    "    - use union categoricals to preserve categories\n",
    "- drop duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path\n",
    "path = 'data/raw/'\n",
    "#get list of data files, ignoring any hidden files in directory \n",
    "datafiles = [file for file in os.listdir(path) if not file.startswith('.')]\n",
    "#init filenumber\n",
    "filenumber = 1\n",
    "#define new col names\n",
    "import_colnames_dict = {'Weight': 'weight',\n",
    "                        'Weight Unit': 'weight_unit',\n",
    "                        'Quantity': 'qty',\n",
    "                        'Quantity Type': 'qty_type',\n",
    "                        'TEUs': 'teus',\n",
    "                        'Estimated Value': 'value_est',\n",
    "                        'Arrival Date': 'date_arrival',\n",
    "                        'Container Piece Count': 'container_piece_count',\n",
    "                        'Quantity of Commodity Short Description': 'commod_short_desc_qty',\n",
    "                        'Territory of Origin': 'origin_territory',\n",
    "                        'Region of Origin': 'origin_region',\n",
    "                        'Port of Arrival Code': 'arrival_port_code',\n",
    "                        'Port of Arrival': 'arrival_port_name',\n",
    "                        'Port of Departure Code': 'departure_port_code',\n",
    "                        'Port of Departure': 'departure_port_name',\n",
    "                        'Final Destination': 'dest_final',\n",
    "                        'Coastal Region': 'coast_region',\n",
    "                        'Clearing District': 'clearing_district',\n",
    "                        'Place of Receipt': 'place_receipt',\n",
    "                        'Shipper': 'shipper_name',\n",
    "                        'Shipper Address': 'shipper_address',\n",
    "                        'Consignee': 'consignee_name',\n",
    "                        'Consignee Address': 'consignee_address',\n",
    "                        'Notify Party': 'notify_party1_name',\n",
    "                        'Notify Party Address': 'notify_party1_address',\n",
    "                        'Also Notify Party': 'notify_party2_name',\n",
    "                        'Also Notify Party Address': 'notify_party2_address',\n",
    "                        'Raw Commodity Description': 'commod_desc_raw',\n",
    "                        'Marks Container Number': 'container_id_marks',\n",
    "                        'Marks Description': 'marks_desc',\n",
    "                        'HS Code': 'hs_code',\n",
    "                        'JOC Code': 'joc_code',\n",
    "                        'Commodity Short Description': 'commod_short_desc',\n",
    "                        'Container Number': 'container_ids',\n",
    "                        'Carrier': 'carrier_name',\n",
    "                        'SCAC': 'carrier_scac',\n",
    "                        'Vessel Name': 'vessel_name',\n",
    "                        'Voyage Number': 'vessel_id',\n",
    "                        'Pre Carrier': 'precarrier',\n",
    "                        'IMO Number': 'imo_num',\n",
    "                        'Inbond Code': 'inbond_code',\n",
    "                        'Mode of Transport': 'transport_mode',\n",
    "                        'Bill of Lading Number': 'bol_id'}\n",
    "#define dtypes\n",
    "import_dtype_dict = {'Weight': 'float64',\n",
    "            'Weight Unit': 'category',\n",
    "            'Quantity': 'float64',\n",
    "            'Quantity Type': 'category',\n",
    "            'TEUs': 'float64',\n",
    "            'Estimated Value': 'float64',\n",
    "            'Arrival Date': 'int64',\n",
    "            'Container Piece Count': 'int64',\n",
    "            'Quantity of Commodity Short Description': 'object',\n",
    "            'Territory of Origin': 'category',\n",
    "            'Region of Origin': 'category',\n",
    "            'Port of Arrival Code': 'category',\n",
    "            'Port of Arrival': 'category',\n",
    "            'Port of Departure Code': 'category',\n",
    "            'Port of Departure': 'category',\n",
    "            'Final Destination': 'category',\n",
    "            'Coastal Region': 'category',\n",
    "            'Clearing District': 'category',\n",
    "            'Place of Receipt': 'category',\n",
    "            'Shipper': 'object',\n",
    "            'Shipper Address': 'object',\n",
    "            'Consignee': 'object',\n",
    "            'Consignee Address': 'object',\n",
    "            'Notify Party': 'object',\n",
    "            'Notify Party Address': 'object',\n",
    "            'Also Notify Party': 'object',\n",
    "            'Also Notify Party Address': 'object',\n",
    "            'Raw Commodity Description': 'object',\n",
    "            'Marks Container Number': 'object',\n",
    "            'Marks Description': 'object',\n",
    "            'HS Code': 'category',\n",
    "            'JOC Code': 'category',\n",
    "            'Commodity Short Description': 'object',\n",
    "            'Container Number': 'object',\n",
    "            'Carrier': 'category',\n",
    "            'SCAC': 'category',\n",
    "            'Vessel Name': 'object',\n",
    "            'Voyage Number': 'object',\n",
    "            'Pre Carrier': 'float64',\n",
    "            'IMO Number': 'float64',\n",
    "            'Inbond Code': 'float64',\n",
    "            'Mode of Transport': 'category',\n",
    "            'Bill of Lading Number': 'object'}\n",
    "#define category variable cols\n",
    "catcols = ['weight_unit', 'qty_type', 'origin_territory', 'origin_region', 'arrival_port_code', \n",
    "           'arrival_port_name', 'departure_port_code', 'departure_port_name', 'dest_final', 'coast_region', \n",
    "           'clearing_district', 'place_receipt', 'hs_code', 'joc_code', 'carrier_name', 'carrier_scac', \n",
    "           'transport_mode']\n",
    "#get col names for reordering\n",
    "import_colnames = list(import_colnames_dict.values())\n",
    "\n",
    "#set filename for complete dataset\n",
    "dataset_filename = 'data/piers_imports_complete.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSV files...\n",
      " Files to process:  39 \n",
      "\n",
      "Extracting file 1...\n",
      "Saving file 1 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 1: 88.54066777229309 sec \n",
      "\n",
      "Extracting file 2...\n",
      "Saving file 2 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 2: 107.71655869483948 sec \n",
      "\n",
      "Extracting file 3...\n",
      "Saving file 3 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 3: 50.84528589248657 sec \n",
      "\n",
      "Extracting file 4...\n",
      "Saving file 4 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 4: 87.93762516975403 sec \n",
      "\n",
      "Extracting file 5...\n",
      "Saving file 5 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 5: 95.53490996360779 sec \n",
      "\n",
      "Extracting file 6...\n",
      "Saving file 6 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 6: 87.1230878829956 sec \n",
      "\n",
      "Extracting file 7...\n",
      "Saving file 7 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 7: 47.309744119644165 sec \n",
      "\n",
      "Extracting file 8...\n",
      "Saving file 8 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 8: 83.01932716369629 sec \n",
      "\n",
      "Extracting file 9...\n",
      "Saving file 9 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 9: 127.27074193954468 sec \n",
      "\n",
      "Extracting file 10...\n",
      "Saving file 10 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 10: 89.12976908683777 sec \n",
      "\n",
      "Extracting file 11...\n",
      "Saving file 11 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 11: 77.85898995399475 sec \n",
      "\n",
      "Extracting file 12...\n",
      "Saving file 12 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 12: 183.92909216880798 sec \n",
      "\n",
      "Extracting file 13...\n",
      "Saving file 13 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 13: 86.54842209815979 sec \n",
      "\n",
      "Extracting file 14...\n",
      "Saving file 14 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 14: 62.20773124694824 sec \n",
      "\n",
      "Extracting file 15...\n",
      "Saving file 15 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 15: 74.53226280212402 sec \n",
      "\n",
      "Extracting file 16...\n",
      "Saving file 16 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 16: 62.025331258773804 sec \n",
      "\n",
      "Extracting file 17...\n",
      "Saving file 17 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 17: 89.44751191139221 sec \n",
      "\n",
      "Extracting file 18...\n",
      "Saving file 18 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 18: 71.33454012870789 sec \n",
      "\n",
      "Extracting file 19...\n",
      "Saving file 19 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 19: 48.60639405250549 sec \n",
      "\n",
      "Extracting file 20...\n",
      "Saving file 20 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 20: 71.46126794815063 sec \n",
      "\n",
      "Extracting file 21...\n",
      "Saving file 21 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 21: 81.94576787948608 sec \n",
      "\n",
      "Extracting file 22...\n",
      "Saving file 22 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 22: 41.76048398017883 sec \n",
      "\n",
      "Extracting file 23...\n",
      "Saving file 23 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 23: 1288.8731439113617 sec \n",
      "\n",
      "Extracting file 24...\n",
      "Saving file 24 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 24: 1017.2591660022736 sec \n",
      "\n",
      "Extracting file 25...\n",
      "Saving file 25 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 25: 77.69774293899536 sec \n",
      "\n",
      "Extracting file 26...\n",
      "Saving file 26 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 26: 89.09298014640808 sec \n",
      "\n",
      "Extracting file 27...\n",
      "Saving file 27 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 27: 70.30713868141174 sec \n",
      "\n",
      "Extracting file 28...\n",
      "Saving file 28 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 28: 53.63289213180542 sec \n",
      "\n",
      "Extracting file 29...\n",
      "Saving file 29 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 29: 94.9303150177002 sec \n",
      "\n",
      "Extracting file 30...\n",
      "Saving file 30 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 30: 83.04269695281982 sec \n",
      "\n",
      "Extracting file 31...\n",
      "Saving file 31 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 31: 76.12779498100281 sec \n",
      "\n",
      "Extracting file 32...\n",
      "Saving file 32 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 32: 93.96154117584229 sec \n",
      "\n",
      "Extracting file 33...\n",
      "Saving file 33 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 33: 46.67009902000427 sec \n",
      "\n",
      "Extracting file 34...\n",
      "Saving file 34 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 34: 62.68845796585083 sec \n",
      "\n",
      "Extracting file 35...\n",
      "Saving file 35 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 35: 87.86052393913269 sec \n",
      "\n",
      "Extracting file 36...\n",
      "Saving file 36 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 36: 53.94966506958008 sec \n",
      "\n",
      "Extracting file 37...\n",
      "Saving file 37 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 37: 86.20633506774902 sec \n",
      "\n",
      "Extracting file 38...\n",
      "Saving file 38 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 38: 82.85877776145935 sec \n",
      "\n",
      "Extracting file 39...\n",
      "Saving file 39 to parquet...\n",
      "Aggregating to full dataset...\n",
      "File complete.\n",
      " Total time for file 39: 47.14834713935852 sec \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Extracting CSV files...\\n', 'Files to process: ', len(datafiles), '\\n')\n",
    "\n",
    "for file in datafiles:\n",
    "    #extract from csv to clean dataframes and concat\n",
    "    start = time.time()\n",
    "    print('Extracting file {}...'.format(filenumber))\n",
    "    #read csv with appropriate dtypes\n",
    "    file_df = dd.read_csv(path+file, dtype=import_dtype_dict, assume_missing=True, sample=1000)\n",
    "    \n",
    "    #rename columns\n",
    "    file_df = file_df.compute().rename(columns=import_colnames_dict)\n",
    "    #unpack strings to list objects\n",
    "    file_df.container_ids = file_df.container_ids.str.split()\n",
    "    file_df.commod_short_desc_qty = file_df.commod_short_desc_qty.str.split(pat=';')\n",
    "    file_df.commod_short_desc = file_df.commod_short_desc.str.split(pat=',')\n",
    "    #recast dates to datetime \n",
    "    file_df.date_arrival = pd.to_datetime(file_df.date_arrival.astype(str), format='%Y%m%d')\n",
    "    #reorder columns\n",
    "    file_df = file_df[import_colnames]\n",
    "    \n",
    "    #save file_df\n",
    "    print('Saving file {} to parquet...'.format(filenumber))\n",
    "    file_df.to_parquet('data/clean_parquet/'+ file[:-3] + 'parquet', engine='fastparquet')\n",
    "\n",
    "    #create/append complete file \n",
    "    print('Aggregating to full dataset...')\n",
    "    if os.path.isfile(dataset_filename):\n",
    "        file_df.to_parquet(dataset_filename, append=True, engine='fastparquet')\n",
    "    else: \n",
    "        file_df.to_parquet(dataset_filename, engine='fastparquet')\n",
    "    end = time.time()\n",
    "\n",
    "    del file_df\n",
    "    print('File complete.\\n', 'Total time for file {}: {} sec \\n'.format(filenumber, end-start))\n",
    "    filenumber += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
