{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIERS Imports ETL\n",
    "\n",
    "This notebook extracts Bill of Lading data from csv files downloaded from S&P Global's PIERS BoL database. Transformations are limited here to setting appropriate datatypes for storage, adding year month and direction columns, and dropping duplicated rows. The data is then saved in .parquet format files by arrival year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import polars as pl #v0.20.7\n",
    "import glob\n",
    "import time\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define schema\n",
    "imports_schema = {'Weight': pl.Float64,\n",
    "            'Weight Unit': pl.Categorical,\n",
    "            'Quantity': pl.Float64,\n",
    "            'Quantity Type': pl.Categorical,\n",
    "            'TEUs': pl.Float64,\n",
    "            'Estimated Value': pl.Float64,\n",
    "            'Arrival Date': pl.Utf8,\n",
    "            'Container Piece Count': pl.Int32,\n",
    "            'Quantity of Commodity Short Description': pl.Utf8,\n",
    "            'Territory of Origin': pl.Categorical,\n",
    "            'Region of Origin': pl.Categorical,\n",
    "            'Port of Arrival Code': pl.Categorical,\n",
    "            'Port of Arrival': pl.Categorical,\n",
    "            'Port of Departure Code': pl.Categorical,\n",
    "            'Port of Departure': pl.Categorical,\n",
    "            'Final Destination': pl.Categorical,\n",
    "            'Coastal Region': pl.Categorical,\n",
    "            'Clearing District': pl.Categorical,\n",
    "            'Place of Receipt': pl.Categorical,\n",
    "            'Shipper': pl.Utf8,\n",
    "            'Shipper Address': pl.Utf8,\n",
    "            'Consignee': pl.Utf8,\n",
    "            'Consignee Address': pl.Utf8,\n",
    "            'Notify Party': pl.Utf8,\n",
    "            'Notify Party Address': pl.Utf8,\n",
    "            'Also Notify Party': pl.Utf8,\n",
    "            'Also Notify Party Address': pl.Utf8,\n",
    "            'Raw Commodity Description': pl.Utf8,\n",
    "            'Marks Container Number': pl.Utf8,\n",
    "            'Marks Description': pl.Utf8,\n",
    "            'HS Code': pl.Utf8,\n",
    "            'JOC Code': pl.Utf8,\n",
    "            'Commodity Short Description': pl.Utf8,\n",
    "            'Container Number': pl.Utf8,\n",
    "            'Carrier': pl.Categorical,\n",
    "            'SCAC': pl.Categorical,\n",
    "            'Vessel Name': pl.Utf8,\n",
    "            'Voyage Number': pl.Utf8,\n",
    "            'Pre Carrier': pl.Float64,\n",
    "            'IMO Number': pl.Int32,\n",
    "            'Inbond Code': pl.Float64,\n",
    "            'Mode of Transport': pl.Categorical,\n",
    "            'Bill of Lading Number': pl.Utf8}\n",
    "\n",
    "#define pythonic column names \n",
    "import_colnames_dict = {'Weight': 'weight',\n",
    "            'Weight Unit': 'weight_unit',\n",
    "            'Quantity': 'qty',\n",
    "            'Quantity Type': 'qty_type',\n",
    "            'TEUs': 'teus',\n",
    "            'Estimated Value': 'value_est',\n",
    "            'Arrival Date': 'date',\n",
    "            'Container Piece Count': 'container_piece_count',\n",
    "            'Quantity of Commodity Short Description': 'commod_short_desc_qty',\n",
    "            'Territory of Origin': 'origin_territory',\n",
    "            'Region of Origin': 'origin_region',\n",
    "            'Port of Arrival Code': 'arrival_port_code',\n",
    "            'Port of Arrival': 'arrival_port_name',\n",
    "            'Port of Departure Code': 'departure_port_code',\n",
    "            'Port of Departure': 'departure_port_name',\n",
    "            'Final Destination': 'dest_final',\n",
    "            'Coastal Region': 'coast_region',\n",
    "            'Clearing District': 'clearing_district',\n",
    "            'Place of Receipt': 'place_receipt',\n",
    "            'Shipper': 'shipper_name',\n",
    "            'Shipper Address': 'shipper_address',\n",
    "            'Consignee': 'consignee_name',\n",
    "            'Consignee Address': 'consignee_address',\n",
    "            'Notify Party': 'notify_party1_name',\n",
    "            'Notify Party Address': 'notify_party1_address',\n",
    "            'Also Notify Party': 'notify_party2_name',\n",
    "            'Also Notify Party Address': 'notify_party2_address',\n",
    "            'Raw Commodity Description': 'commod_desc_raw',\n",
    "            'Marks Container Number': 'container_id_marks',\n",
    "            'Marks Description': 'marks_desc',\n",
    "            'HS Code': 'hs_code',\n",
    "            'JOC Code': 'joc_code',\n",
    "            'Commodity Short Description': 'commod_short_desc',\n",
    "            'Container Number': 'container_ids',\n",
    "            'Carrier': 'carrier_name',\n",
    "            'SCAC': 'carrier_scac',\n",
    "            'Vessel Name': 'vessel_name',\n",
    "            'Voyage Number': 'voyage_number',\n",
    "            'Pre Carrier': 'precarrier',\n",
    "            'IMO Number': 'vessel_id',\n",
    "            'Inbond Code': 'inbond_code',\n",
    "            'Mode of Transport': 'transport_mode',\n",
    "            'Bill of Lading Number': 'bol_number'}\n",
    "\n",
    "#define years\n",
    "years = pl.arange(2005,2025, eager=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path\n",
    "path = 'data/raw_csv/imports/'\n",
    "#set files and order\n",
    "import_files = glob.glob(path+'*')\n",
    "import_files.sort()\n",
    "\n",
    "#identify columns from new format that are not necessary - these will be dropped\n",
    "dropcolumns = [\n",
    "    'Shipper City', 'Also Notify Party State', 'US Destination City', \n",
    "    'Also Notify Party City', 'US Destination State', 'Consignee State', \n",
    "    'Consignee City', 'Shipper State', 'Notify Party State', 'Notify Party City'\n",
    "]\n",
    "\n",
    "#process data from each file in folder (lazy mode)\n",
    "for i in range(len(import_files)):\n",
    "    #scan csv\n",
    "    im_lf = pl.scan_csv(import_files[i], infer_schema_length=0)\n",
    "    #coerse new format to match old format\n",
    "    if set(dropcolumns).issubset(set(im_lf.columns)):\n",
    "        im_lf = im_lf.drop(dropcolumns)\n",
    "    #process file\n",
    "    im_lf = (\n",
    "        im_lf\n",
    "        #strip whitespace and replace empty str with null\n",
    "        .with_columns(pl.all().str.strip_chars().replace('',None))\n",
    "        #set schema\n",
    "        .cast(imports_schema)\n",
    "        #rename cols\n",
    "        .rename(import_colnames_dict)\n",
    "        #reorder columns\n",
    "        .select(import_colnames_dict.values())\n",
    "        #cast date col to datetime\n",
    "        .with_columns(pl.col('date').str.to_datetime('%Y%m%d'))\n",
    "        .with_columns(\n",
    "            #create direction column\n",
    "            pl.lit('import').cast(pl.Categorical).alias('direction'),\n",
    "            #create bol_id\n",
    "            (pl.col('carrier_scac').fill_null('')+'_'+pl.col('bol_number')).alias('bol_id'),\n",
    "            #extract year\n",
    "            pl.col('date').dt.year().alias('year'),\n",
    "            #extract month (e.g., '202304')\n",
    "            pl.col('date').dt.strftime('%Y%m').alias('month'),\n",
    "            #create lane_id\n",
    "            (pl.col('departure_port_code').cast(pl.Utf8)+'_'+pl.col('arrival_port_code').cast(pl.Utf8))\n",
    "            .cast(pl.Categorical)\n",
    "            .alias('lane_id'),\n",
    "            #convert zero volume values to null\n",
    "            pl.col('teus').replace(0,None),\n",
    "            pl.col('weight').replace(0,None),\n",
    "            pl.col('qty').replace(0,None)\n",
    "        )\n",
    "    )\n",
    "    if i == 0:\n",
    "        #create main lazyframe\n",
    "        imports_lf = im_lf\n",
    "    else:\n",
    "        #concat file frame to main lazyframe\n",
    "        imports_lf = (\n",
    "            pl.concat([imports_lf, im_lf], how='diagonal')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting observations from 2020...\n",
      "2020 dataframe collected. \n",
      "Writing to parquet...\n",
      "Total Time: 4.196024 minutes\n",
      "Collecting observations from 2021...\n",
      "2021 dataframe collected. \n",
      "Writing to parquet...\n",
      "Total Time: 4.895440 minutes\n",
      "Collecting observations from 2022...\n",
      "2022 dataframe collected. \n",
      "Writing to parquet...\n",
      "Total Time: 5.137726 minutes\n",
      "Collecting observations from 2023...\n",
      "2023 dataframe collected. \n",
      "Writing to parquet...\n",
      "Total Time: 4.736244 minutes\n",
      "Collecting observations from 2024...\n",
      "2024 dataframe collected. \n",
      "Writing to parquet...\n",
      "Total Time: 4.549712 minutes\n",
      "\n",
      "Imports ETL complete.\n"
     ]
    }
   ],
   "source": [
    "#collect each year and write to parquet \n",
    "for year in years:\n",
    "    print('Collecting observations from '+str(year)+'...')\n",
    "    start = time.time()\n",
    "    df = (\n",
    "        imports_lf\n",
    "        #filter by year\n",
    "        .filter(pl.col('date').str.starts_with(str(year)))\n",
    "        #drop duplicates\n",
    "        .unique()\n",
    "        #collect\n",
    "        .collect()\n",
    "    )\n",
    "    #print status\n",
    "    print(str(year)+' dataframe collected. \\nWriting to parquet...')\n",
    "\n",
    "    #write file to parquet\n",
    "    df.write_parquet(file='data/raw_parquet/imports/piers_imports_'+str(year)+'_raw.parquet')\n",
    "    #print status\n",
    "    print('Total Time: {:2f} minutes'.format((time.time()-start)/60))\n",
    "    del df\n",
    "\n",
    "print('\\nImports ETL complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close string cache\n",
    "pl.disable_string_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
