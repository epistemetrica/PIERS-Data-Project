{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIERS Container BOL Data ETL \n",
    "\n",
    "This notebook builds an ETL pipeline for S&P Global's PIERS data. Data is extracted from CSV files downloaded from the Global Trade Analytics Suite, assigned appropriate datatypes, concatendated into a single dataframe, and loaded to an Apache Parquet file for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform\n",
    "\n",
    "Read from csv into a polars dataframe with appropriate dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/sgfd2dls28n4y4v4_jw03ly40000gp/T/ipykernel_24928/2458340736.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/raw/exports/PIERS export records 2010 01-06 874C48B941A54798B9047F6C51E0EC42.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/raw/exports/PIERS export records 2010 01-06 874C48B941A54798B9047F6C51E0EC42.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1871895 entries, 0 to 1871894\n",
      "Data columns (total 30 columns):\n",
      " #   Column                                   Dtype  \n",
      "---  ------                                   -----  \n",
      " 0   Shipper                                  object \n",
      " 1   Shipper Address                          object \n",
      " 2   Weight                                   float64\n",
      " 3   Weight Unit                              object \n",
      " 4   Quantity                                 float64\n",
      " 5   Quantity Type                            object \n",
      " 6   TEUs                                     float64\n",
      " 7   Carrier                                  object \n",
      " 8   SCAC                                     object \n",
      " 9   Vessel Name                              object \n",
      " 10  Voyage Number                            object \n",
      " 11  Bill of Lading Number                    object \n",
      " 12  IMO Number                               float64\n",
      " 13  Estimated Value                          float64\n",
      " 14  Port of Departure Code                   float64\n",
      " 15  Port of Departure                        object \n",
      " 16  Container Number                         object \n",
      " 17  Container Piece Count                    int64  \n",
      " 18  Coastal Region                           object \n",
      " 19  Raw Commodity Description                object \n",
      " 20  Commodity Short Description              object \n",
      " 21  HS Code                                  object \n",
      " 22  JOC Code                                 object \n",
      " 23  Quantity of Commodity Short Description  object \n",
      " 24  Departure Date                           int64  \n",
      " 25  U.S. Origin                              object \n",
      " 26  Destination Territory                    object \n",
      " 27  Destination Region                       object \n",
      " 28  Declared Destination Port Code           float64\n",
      " 29  Declared Destination Port                object \n",
      "dtypes: float64(7), int64(2), object(21)\n",
      "memory usage: 428.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "could not parse `1685.62` as dtype `i64` at column 'Weight' (column number 3)\n\nThe current offset in the file is 53992750 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `dtypes` argument\n- setting `ignore_errors` to `True`,\n- adding `1685.62` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pldf \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/raw/exports/PIERS export records 2010 01-06 874C48B941A54798B9047F6C51E0EC42.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mto_pandas(pldf)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsu/lib/python3.12/site-packages/polars/lazyframe/frame.py:1706\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, _eager)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     comm_subplan_elim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m ldf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf\u001b[38;5;241m.\u001b[39moptimization_toggle(\n\u001b[1;32m   1696\u001b[0m     type_coercion,\n\u001b[1;32m   1697\u001b[0m     predicate_pushdown,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m     _eager,\n\u001b[1;32m   1705\u001b[0m )\n\u001b[0;32m-> 1706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mComputeError\u001b[0m: could not parse `1685.62` as dtype `i64` at column 'Weight' (column number 3)\n\nThe current offset in the file is 53992750 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `dtypes` argument\n- setting `ignore_errors` to `True`,\n- adding `1685.62` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```"
     ]
    }
   ],
   "source": [
    "pldf = (\n",
    "    pl.scan_csv('data/raw/exports/PIERS export records 2010 01-06 874C48B941A54798B9047F6C51E0EC42.csv', infer_schema_length=10000)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "df = pl.to_pandas(pldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path\n",
    "path = 'data/raw/'\n",
    "#get list of data files, ignoring any hidden files in directory \n",
    "datafiles = [file for file in os.listdir(path) if not file.startswith('.')]\n",
    "#init filenumber\n",
    "filenumber = 1\n",
    "#define new col names\n",
    "import_colnames_dict = {'Weight': 'weight',\n",
    "                        'Weight Unit': 'weight_unit',\n",
    "                        'Quantity': 'qty',\n",
    "                        'Quantity Type': 'qty_type',\n",
    "                        'TEUs': 'teus',\n",
    "                        'Estimated Value': 'value_est',\n",
    "                        'Arrival Date': 'date_arrival',\n",
    "                        'Container Piece Count': 'container_piece_count',\n",
    "                        'Quantity of Commodity Short Description': 'commod_short_desc_qty',\n",
    "                        'Territory of Origin': 'origin_territory',\n",
    "                        'Region of Origin': 'origin_region',\n",
    "                        'Port of Arrival Code': 'arrival_port_code',\n",
    "                        'Port of Arrival': 'arrival_port_name',\n",
    "                        'Port of Departure Code': 'departure_port_code',\n",
    "                        'Port of Departure': 'departure_port_name',\n",
    "                        'Final Destination': 'dest_final',\n",
    "                        'Coastal Region': 'coast_region',\n",
    "                        'Clearing District': 'clearing_district',\n",
    "                        'Place of Receipt': 'place_receipt',\n",
    "                        'Shipper': 'shipper_name',\n",
    "                        'Shipper Address': 'shipper_address',\n",
    "                        'Consignee': 'consignee_name',\n",
    "                        'Consignee Address': 'consignee_address',\n",
    "                        'Notify Party': 'notify_party1_name',\n",
    "                        'Notify Party Address': 'notify_party1_address',\n",
    "                        'Also Notify Party': 'notify_party2_name',\n",
    "                        'Also Notify Party Address': 'notify_party2_address',\n",
    "                        'Raw Commodity Description': 'commod_desc_raw',\n",
    "                        'Marks Container Number': 'container_id_marks',\n",
    "                        'Marks Description': 'marks_desc',\n",
    "                        'HS Code': 'hs_code',\n",
    "                        'JOC Code': 'joc_code',\n",
    "                        'Commodity Short Description': 'commod_short_desc',\n",
    "                        'Container Number': 'container_ids',\n",
    "                        'Carrier': 'carrier_name',\n",
    "                        'SCAC': 'carrier_scac',\n",
    "                        'Vessel Name': 'vessel_name',\n",
    "                        'Voyage Number': 'vessel_id',\n",
    "                        'Pre Carrier': 'precarrier',\n",
    "                        'IMO Number': 'imo_num',\n",
    "                        'Inbond Code': 'inbond_code',\n",
    "                        'Mode of Transport': 'transport_mode',\n",
    "                        'Bill of Lading Number': 'bol_id'}\n",
    "#define dtypes\n",
    "import_dtype_dict = {'Weight': 'float64',\n",
    "            'Weight Unit': 'category',\n",
    "            'Quantity': 'float64',\n",
    "            'Quantity Type': 'category',\n",
    "            'TEUs': 'float64',\n",
    "            'Estimated Value': 'float64',\n",
    "            'Arrival Date': 'int64',\n",
    "            'Container Piece Count': 'int64',\n",
    "            'Quantity of Commodity Short Description': 'object',\n",
    "            'Territory of Origin': 'category',\n",
    "            'Region of Origin': 'category',\n",
    "            'Port of Arrival Code': 'category',\n",
    "            'Port of Arrival': 'category',\n",
    "            'Port of Departure Code': 'category',\n",
    "            'Port of Departure': 'category',\n",
    "            'Final Destination': 'category',\n",
    "            'Coastal Region': 'category',\n",
    "            'Clearing District': 'category',\n",
    "            'Place of Receipt': 'category',\n",
    "            'Shipper': 'object',\n",
    "            'Shipper Address': 'object',\n",
    "            'Consignee': 'object',\n",
    "            'Consignee Address': 'object',\n",
    "            'Notify Party': 'object',\n",
    "            'Notify Party Address': 'object',\n",
    "            'Also Notify Party': 'object',\n",
    "            'Also Notify Party Address': 'object',\n",
    "            'Raw Commodity Description': 'object',\n",
    "            'Marks Container Number': 'object',\n",
    "            'Marks Description': 'object',\n",
    "            'HS Code': 'category',\n",
    "            'JOC Code': 'category',\n",
    "            'Commodity Short Description': 'object',\n",
    "            'Container Number': 'object',\n",
    "            'Carrier': 'category',\n",
    "            'SCAC': 'category',\n",
    "            'Vessel Name': 'object',\n",
    "            'Voyage Number': 'object',\n",
    "            'Pre Carrier': 'float64',\n",
    "            'IMO Number': 'float64',\n",
    "            'Inbond Code': 'float64',\n",
    "            'Mode of Transport': 'category',\n",
    "            'Bill of Lading Number': 'object'}\n",
    "#define category variable cols\n",
    "catcols = ['weight_unit', 'qty_type', 'origin_territory', 'origin_region', 'arrival_port_code', \n",
    "           'arrival_port_name', 'departure_port_code', 'departure_port_name', 'dest_final', 'coast_region', \n",
    "           'clearing_district', 'place_receipt', 'hs_code', 'joc_code', 'carrier_name', 'carrier_scac', \n",
    "           'transport_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSV files...\n",
      " Files to process:  39 \n",
      "\n",
      "Extracting file  1 ...\n",
      "Extraction complete.\n",
      " Time: 30.30369281768799 sec \n",
      "\n",
      "Extracting file  2 ...\n",
      "Extraction complete.\n",
      " Time: 38.606818199157715 sec \n",
      "\n",
      "Extracting file  3 ...\n",
      "Extraction complete.\n",
      " Time: 20.177419900894165 sec \n",
      "\n",
      "Extracting file  4 ...\n",
      "Extraction complete.\n",
      " Time: 33.34604597091675 sec \n",
      "\n",
      "Extracting file  5 ...\n",
      "Extraction complete.\n",
      " Time: 34.10702681541443 sec \n",
      "\n",
      "Extracting file  6 ...\n",
      "Extraction complete.\n",
      " Time: 33.43861198425293 sec \n",
      "\n",
      "Extracting file  7 ...\n",
      "Extraction complete.\n",
      " Time: 20.76675009727478 sec \n",
      "\n",
      "Extracting file  8 ...\n",
      "Extraction complete.\n",
      " Time: 38.86877512931824 sec \n",
      "\n",
      "Extracting file  9 ...\n",
      "Extraction complete.\n",
      " Time: 47.7787709236145 sec \n",
      "\n",
      "Extracting file  10 ...\n",
      "Extraction complete.\n",
      " Time: 47.76598501205444 sec \n",
      "\n",
      "Extracting file  11 ...\n",
      "Extraction complete.\n",
      " Time: 36.88781189918518 sec \n",
      "\n",
      "Extracting file  12 ...\n",
      "Extraction complete.\n",
      " Time: 68.1502730846405 sec \n",
      "\n",
      "Extracting file  13 ...\n",
      "Extraction complete.\n",
      " Time: 59.50603485107422 sec \n",
      "\n",
      "Extracting file  14 ...\n",
      "Extraction complete.\n",
      " Time: 55.72740697860718 sec \n",
      "\n",
      "Extracting file  15 ...\n",
      "Extraction complete.\n",
      " Time: 112.34333205223083 sec \n",
      "\n",
      "Extracting file  16 ...\n",
      "Extraction complete.\n",
      " Time: 71.49107193946838 sec \n",
      "\n",
      "Extracting file  17 ...\n",
      "Extraction complete.\n",
      " Time: 101.57660007476807 sec \n",
      "\n",
      "Extracting file  18 ...\n",
      "Extraction complete.\n",
      " Time: 122.36245512962341 sec \n",
      "\n",
      "Extracting file  19 ...\n",
      "Extraction complete.\n",
      " Time: 124.83687400817871 sec \n",
      "\n",
      "Extracting file  20 ...\n",
      "Extraction complete.\n",
      " Time: 286.40643310546875 sec \n",
      "\n",
      "Extracting file  21 ...\n",
      "Extraction complete.\n",
      " Time: 212.7979221343994 sec \n",
      "\n",
      "Extracting file  22 ...\n",
      "Extraction complete.\n",
      " Time: 212.2327618598938 sec \n",
      "\n",
      "Extracting file  23 ...\n",
      "Extraction complete.\n",
      " Time: 261.81143975257874 sec \n",
      "\n",
      "Extracting file  24 ...\n",
      "Extraction complete.\n",
      " Time: 550.404541015625 sec \n",
      "\n",
      "Extracting file  25 ...\n",
      "Extraction complete.\n",
      " Time: 339.14352011680603 sec \n",
      "\n",
      "Extracting file  26 ...\n",
      "Extraction complete.\n",
      " Time: 1280.0292139053345 sec \n",
      "\n",
      "Extracting file  27 ...\n",
      "Extraction complete.\n",
      " Time: 438.097505569458 sec \n",
      "\n",
      "Extracting file  28 ...\n",
      "Extraction complete.\n",
      " Time: 418.41711807250977 sec \n",
      "\n",
      "Extracting file  29 ...\n",
      "Extraction complete.\n",
      " Time: 481.21947026252747 sec \n",
      "\n",
      "Extracting file  30 ...\n",
      "Extraction complete.\n",
      " Time: 919.8763341903687 sec \n",
      "\n",
      "Extracting file  31 ...\n",
      "Extraction complete.\n",
      " Time: 642.8634748458862 sec \n",
      "\n",
      "Extracting file  32 ...\n",
      "Extraction complete.\n",
      " Time: 695.3986430168152 sec \n",
      "\n",
      "Extracting file  33 ...\n",
      "Extraction complete.\n",
      " Time: 1586.8421077728271 sec \n",
      "\n",
      "Extracting file  34 ...\n",
      "Extraction complete.\n",
      " Time: 1804.94468998909 sec \n",
      "\n",
      "Extracting file  35 ...\n",
      "Extraction complete.\n",
      " Time: 4537.186676263809 sec \n",
      "\n",
      "Extracting file  36 ...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('Extracting CSV files...\\n', 'Files to process: ', len(datafiles), '\\n')\n",
    "\n",
    "#extract from csv to clean dataframes and concat\n",
    "for filename in datafiles:\n",
    "    start = time.time()\n",
    "    print('Extracting file ', filenumber, '...')\n",
    "    #read csv with appropriate dtypes\n",
    "    file_df = pd.read_csv(path+filename, dtype=import_dtype_dict)\n",
    "    #rename columns\n",
    "    file_df.rename(columns=import_colnames_dict, inplace=True)\n",
    "    #unpack strings to list objects\n",
    "    file_df.container_ids = file_df.container_ids.str.split()\n",
    "    file_df.commod_short_desc_qty = file_df.commod_short_desc_qty.str.split(pat=';')\n",
    "    file_df.commod_short_desc = file_df.commod_short_desc.str.split(pat=',')\n",
    "    #recast dates to datetime \n",
    "    file_df.date_arrival = pd.to_datetime(file_df.date_arrival.astype(str), format='%Y%m%d') \n",
    "    #concat to or create main imports df\n",
    "    if 'imports_df' in locals():\n",
    "        #create category unions and assign union to each df col\n",
    "        for col in catcols:\n",
    "            catunion = pd.api.types.union_categoricals([imports_df[col], file_df[col]])\n",
    "            imports_df[col] = pd.Categorical(imports_df[col], categories=catunion.categories)\n",
    "            file_df[col] = pd.Categorical(file_df[col], categories=catunion.categories)\n",
    "        #concat to main df\n",
    "        imports_df = pd.concat([imports_df, file_df])\n",
    "    else:\n",
    "        imports_df = file_df \n",
    "    del file_df\n",
    "    end = time.time()\n",
    "    print('Extraction complete.\\n', 'Time: {} sec \\n'.format(end-start))\n",
    "    filenumber += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect output \n",
    "display(imports_df.head())\n",
    "imports_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet file\n",
    "imports_df.to_parquet('data/piers_imports.parquet', index=False, engine='fastparquet') #requires fastparquet dependency  \n",
    "\n",
    "#delete imports df\n",
    "del imports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_parquet('data/piers_imports.parquet', engine='fastparquet')\n",
    "imports_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
